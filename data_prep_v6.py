import os
import numpy as np
import sentencepiece as spm # Tiktoken yerine SentencePiece
from datasets import load_dataset
from tqdm import tqdm
import json
import html
import unicodedata

# -----------------------------------------------------------------------------
# AYARLAR
DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), 'data', 'platinum_tr')
TOKENIZER_MODEL = 'tokenizer/tr_unigram32k.model' # Senin model dosyan

os.makedirs(DATA_CACHE_DIR, exist_ok=True)

# 1. Tokenizer'Ä± YÃ¼kle
if not os.path.exists(TOKENIZER_MODEL):
    raise FileNotFoundError(f"HATA: {TOKENIZER_MODEL} dosyasÄ± bulunamadÄ±! LÃ¼tfen script ile aynÄ± klasÃ¶re koy.")

print(f"ğŸ“– Tokenizer yÃ¼kleniyor: {TOKENIZER_MODEL}")
sp = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL)
vocab_size = sp.get_piece_size()
print(f"   Vocab Size: {vocab_size}")

# EOT (End of Text) Token belirlemesi
# Genelde SentencePiece'de EOS id kullanÄ±lÄ±r (genelde 2 veya 1'dir)
EOT_TOKEN = sp.eos_id() 
if EOT_TOKEN == -1: EOT_TOKEN = 2 # EÄŸer tanÄ±mlÄ± deÄŸilse manuel atanÄ±r

def clean_text(text):
    if not text: return ""
    # HTML karakterlerini dÃ¼zelt
    text = html.unescape(str(text)).replace("\xa0", " ")
    # Unicode normalizasyonu
    text = unicodedata.normalize("NFKC", text)
    return text.strip()

# Veri ToplayÄ±cÄ± Liste
all_token_ids = []

# --- 1. COSMOS DATASET (HuggingFace) ---
print("ğŸ“¥ COSMOS Dataset Ä°ndiriliyor...")
try:
    ds_cosmos = load_dataset("Berkesule/COSMOS-Sentetic-Turkish-Corpus-2GB-Clean", split="train", streaming=True)
    for item in tqdm(ds_cosmos, desc="COSMOS Tokenize"):
        text = clean_text(item.get('text', ''))
        if len(text) > 20:
            # SENTENCEPIECE ile tokenize et
            tokens = sp.EncodeAsIds(text)
            all_token_ids.extend(tokens)
            all_token_ids.append(EOT_TOKEN) 
except Exception as e:
    print(f"âŒ Hata (COSMOS): {e}")

# --- 2. MY GOLD DATASET (Ã–nceki sentetik verin) ---
gold_path = "my_gold_dataset.jsonl"
if os.path.exists(gold_path):
    print("ğŸ“¥ Gold Dataset Ä°ÅŸleniyor...")
    with open(gold_path, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="Gold Data"):
            try:
                d = json.loads(line)
                # Soru-Cevap formatÄ±nÄ± metne dÃ¶kÃ¼yoruz
                text = f"Soru: {clean_text(d.get('instruction',''))}\nCevap: {clean_text(d.get('output',''))}"
                tokens = sp.EncodeAsIds(text)
                all_token_ids.extend(tokens)
                all_token_ids.append(EOT_TOKEN)
            except: pass

# --- 3. WIKI QA ---
try:
    print("ğŸ“¥ Wiki QA Ä°ÅŸleniyor...")
    ds_wiki = load_dataset("avometre/turkish-wikipedia-qa", split="train", streaming=True)
    for item in tqdm(ds_wiki, desc="Wiki QA"):
        q = clean_text(item.get('question', ''))
        a = clean_text(item.get('answer', ''))
        text = f"Soru: {q}\nCevap: {a}"
        all_token_ids.extend(sp.EncodeAsIds(text))
        all_token_ids.append(EOT_TOKEN)
except: print("âŒ Hata (Wiki)")

# --- 4. ALPACA ---
try:
    print("ğŸ“¥ Alpaca Ä°ÅŸleniyor...")
    ds_alpaca = load_dataset("TFLai/Turkish-Alpaca", split="train", streaming=True)
    for item in tqdm(ds_alpaca, desc="Alpaca"):
        q = clean_text(item.get('instruction', ''))
        a = clean_text(item.get('output', ''))
        text = f"Soru: {q}\nCevap: {a}"
        all_token_ids.extend(sp.EncodeAsIds(text))
        all_token_ids.append(EOT_TOKEN)
except: print("âŒ Hata (Alpaca)")


# --- KAYIT Ä°ÅLEMÄ° (.bin dosyalarÄ±) ---
total_tokens = len(all_token_ids)
print(f"ğŸ“Š Toplam Token SayÄ±sÄ±: {total_tokens:,}")
print("ğŸ’¾ Binary dosyalara yazÄ±lÄ±yor...")

# uint16: 0-65535 arasÄ± sayÄ± tutar.
all_tokens_np = np.array(all_token_ids, dtype=np.uint16)

# Train / Val Split
# Knowledge injectionda verinin Ã§oÄŸunu train'e ayÄ±rÄ±rÄ±z.
n = len(all_tokens_np)
train_data = all_tokens_np[:int(n*0.95)]
val_data = all_tokens_np[int(n*0.95):]

# DosyalarÄ± yaz
train_data.tofile(os.path.join(DATA_CACHE_DIR, 'train.bin'))
val_data.tofile(os.path.join(DATA_CACHE_DIR, 'val.bin'))

# Meta dosyasÄ±nÄ± da (vocab size vb.) kaydedelim ki train.py okuyabilsin
meta = {
    'vocab_size': vocab_size,
    'tokenizer': 'sentencepiece',
    'model_file': TOKENIZER_MODEL
}
import pickle
with open(os.path.join(DATA_CACHE_DIR, 'meta.pkl'), 'wb') as f:
    pickle.dump(meta, f)

print(f"âœ… HazÄ±r! Dosyalar burada: {DATA_CACHE_DIR}")
print(f"   Train tokens: {len(train_data):,}")
print(f"   Val tokens: {len(val_data):,}")
